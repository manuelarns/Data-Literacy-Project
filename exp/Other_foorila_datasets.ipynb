{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does it make sense to include the other datasets collected by foorila?\n",
    "\n",
    "Apart from the remote salary dataset, Foorila also collects a devops, ai and infosec salary dataset with the same attributes. As the remote salary dataset only contains <2000 rows, it would be interesting to get these additional datapoints. \n",
    "\n",
    "It turns out that the additional datasets contain mostly duplicates of the remote dataset. Only 17% (ai), 22% (infosec) and 12% of the devops-dataset are new rows. As it is unclear how this is handeled internally by foorila, we will not include these additional datapoints in our further analysis\n",
    "\n",
    "Tim Weisbarth January 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ai_salaries': 0.3906810035842294, 'infosec_salaries': 0.49663299663299665, 'devops_salaries': 0.37883008356545966}\n"
     ]
    }
   ],
   "source": [
    "#import modules\n",
    "import pandas as pd\n",
    "\n",
    "# Load all datasets provided by foorila\n",
    "sal_devops = pd.read_csv('../data/salaries_devops.csv')\n",
    "sal_infosec = pd.read_csv('../data/salaries_infosec.csv')\n",
    "sal_ai = pd.read_csv('../data/salaries_ai.csv')\n",
    "sal_remote = pd.read_csv('../data/salaries.csv')\n",
    "\n",
    "#Make work_year attribute compatible\n",
    "for i in [\"2021e\", \"2022e\"]:\n",
    "        sal_remote.loc[(sal_remote.work_year == i), \"work_year\"]= int(i[:-1])\n",
    "sal_remote.astype({'work_year': 'int64' })\n",
    "\n",
    "# Merge remote salaries with the respective other dataset\n",
    "attr =  ['work_year','experience_level','employment_type','job_title','salary','salary_currency',\n",
    "        'salary_in_usd','employee_residence','remote_ratio','company_location','company_size'] \n",
    "\n",
    "sim_ai = pd.merge(sal_ai, sal_remote.drop_duplicates(), on=attr)\n",
    "sim_infosec = pd.merge(sal_infosec, sal_remote.drop_duplicates(), on=attr)\n",
    "sim_devops = pd.merge(sal_devops, sal_remote.drop_duplicates(), left_on=attr, right_on=attr)\n",
    "\n",
    "# Print percentage of rows in the other datasets \n",
    "# that are not present in the remote salary dataset already\n",
    "dic = {\"ai_salaries\" :      1 - sim_ai.shape[0]/sal_ai.shape[0],\n",
    "       \"infosec_salaries\" : 1 - sim_infosec.shape[0]/sal_infosec.shape[0],\n",
    "       \"devops_salaries\" :  1 - sim_devops.shape[0] / sal_devops.shape[0]    \n",
    "      }\n",
    "print(dic)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eafa29cded688e2ad01a641af16ee15b5bc5261ac28f50c6fd144fc2116ba3b5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('geo-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
